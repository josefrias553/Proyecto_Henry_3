# Marketplace Rentals ELT Pipeline

**Versi√≥n:** 1.3.0  
**Estado:** Activo (Fase Bronze)

Este proyecto implementa un pipeline ELT (Extract, Load, Transform) robusto y modular para la ingesta y procesamiento de datos de un marketplace de alquileres. Soporta m√∫ltiples fuentes de datos (APIs, Web Scraping, Archivos CSV locales) y utiliza **PySpark** para transformaciones escalables en AWS S3.

---

## üèóÔ∏è Arquitectura

El pipeline sigue una arquitectura de Data Lake por capas:

![alt text](img\image.png)

### Capas del Data Lake
- **Raw:** Datos crudos tal cual llegan de la fuente (JSON, CSV, HTML).
- **Bronze:** Datos limpios, tipados, deduplicados y en formato **Parquet**.
- **Silver (En progreso):** Datos enriquecidos, joineados y agregados.
- **Gold (Futuro):** Modelos dimensionales y KPIs de negocio.

---

## üöÄ Instalaci√≥n y Configuraci√≥n

### Pre-requisitos
- **Python 3.11+**
- **Java 17** (Requerido para PySpark)
- **AWS CLI** configurado
- **Docker** (Opcional, para ejecuci√≥n contenerizada)

### 1. Configurar Entorno Virtual

```bash
# Crear entorno
python -m venv venv

# Activar (Windows)
.\venv\Scripts\Activate

# Activar (Linux/Mac)
source venv/bin/activate
```

### 2. Instalar Dependencias

```bash
pip install -r requirements.txt
```

### 3. Variables de Entorno (.env)

Crear un archivo `.env` basado en `.env.example`:

```ini
# AWS Credentials
AWS_ACCESS_KEY_ID=tu_access_key
AWS_SECRET_ACCESS_KEY=tu_secret_key
AWS_REGION=us-east-1

# S3 Buckets (Producci√≥n)
RAW_BUCKET=data-lake-raw-henry
BRONZE_BUCKET=data-lake-bronze-henry

# S3 Buckets (Verificaci√≥n)
RAW_BUCKET_VERIFICATION=data-lake-raw-verification-bucket
BRONZE_BUCKET_VERIFICATION=data-lake-bronze-verification-bucket

# Configuraci√≥n General
EXECUTION_MODE=verification  # Controls data validation rules (verification=strict, production=lenient)
```

---

## üì¶ Entidades Soportadas

| Entidad | Fuente | Tipo | Descripci√≥n | Ruta S3 |
|---------|--------|------|-------------|---------|
| `listings` | DummyJSON | API REST | Listings de propiedades | `api/listings/` |
| `posts` | DummyJSON | API REST | Blog posts de usuarios | `api/posts/` |
| `neighborhoods` | Wikipedia | Scraping | Datos de barrios | `scraping/neighborhoods/` |
| `productos` | CSV Local | Archivo | Inventario de productos | `csv/productos/` |
| **`nyc`** | CSV Local | Archivo | **Airbnb NYC (48K+ rows)** | `csv/nyc/` |

---

## üíª Ejemplos de Ejecuci√≥n

## Ingesta de Datos (Capa Raw)

El proceso de extracci√≥n (API/Scraping/CSV) tambi√©n se puede ejecutar v√≠a Docker para consistencia.

### 1. Ingesta desde API (Listings & Posts)
```bash
# Listings (DummyJSON)
docker-compose run --rm --entrypoint python elt-spark -m src.main --source api --entity listings

# Posts (JSONPlaceholder)
docker-compose run --rm --entrypoint python elt-spark -m src.main --source api --entity posts --api-url https://jsonplaceholder.typicode.com/posts
```

### 2. Ingesta desde CSV (NYC)
Los archivos CSV deben estar en la carpeta `datos/nyc` local, la cual se debe montar o copiar. 
*Nota: Para carga simple de CSVs, se recomienda subir directamente a S3 o usar el script `src.main` localmente si se tiene Python instalado.*

```bash
python -m src.main --source csv --entity nyc
```

---

## üìä Preguntas de Negocio (Caso NYC)

El pipeline de NYC est√° dise√±ado para responder:

1.  **Precios:** ¬øCu√°l es el precio promedio por distrito? (Manhattan vs Bronx).
2.  **Oferta:** ¬øQu√© tipo de habitaci√≥n domina el mercado?
3.  **Top Hosts:** ¬øQui√©nes son los anfitriones con m√°s propiedades?
4.  **Ocupaci√≥n:** Segmentaci√≥n de listings por disponibilidad anual (High/Low demand).
5.  **Geograf√≠a:** Barrios con mayor densidad de oferta activa.

---

## üìÇ Estructura del Proyecto

```text
M3/
‚îú‚îÄ‚îÄ bronze_output/          # Salida local de transformaciones local (Mapped to /app/bronze_output in Docker)
‚îú‚îÄ‚îÄ datos/                  # Datos de entrada locales (CSVs)
‚îú‚îÄ‚îÄ scripts/                # Scripts de utilidad
‚îÇ   ‚îú‚îÄ‚îÄ deploy_emr.py       # Deploy en EMR
‚îÇ   ‚îî‚îÄ‚îÄ upload_to_s3.py     # Carga de resultados locales a S3
‚îú‚îÄ‚îÄ src/                    # C√≥digo fuente
‚îÇ   ‚îú‚îÄ‚îÄ extractors/         # L√≥gica de extracci√≥n (API, CSV, Scraping)
‚îÇ   ‚îú‚îÄ‚îÄ utils/              # Utilidades compartidas (Spark, Logs)
‚îÇ   ‚îî‚îÄ‚îÄ main.py             # Punto de entrada principal
‚îú‚îÄ‚îÄ transformations/        # Scripts Spark de transformaci√≥n
‚îÇ   ‚îú‚îÄ‚îÄ bronze/             # L√≥gica Raw -> Bronze (NYC, Listings, Posts)
‚îÇ   ‚îî‚îÄ‚îÄ validation/         # Scripts de validaci√≥n de datos
‚îú‚îÄ‚îÄ Dockerfile              # Definici√≥n de imagen (Python 3.11 + Java 17)
‚îú‚îÄ‚îÄ docker-compose.yml      # Orquestaci√≥n de contenedores
‚îú‚îÄ‚îÄ requirements.txt        # Dependencias de Python
‚îî‚îÄ‚îÄ README.md               # Documentaci√≥n del proyecto
```

---

## üõ†Ô∏è Desarrollo Tecnol√≥gico

-   **Lenguaje:** Python 3.11
-   **Procesamiento:** Apache Spark (PySpark 3.5.2)
-   **Cloud:** AWS S3 (Storage)
-   **Librer√≠as Clave:** `boto3`, `pandas`, `requests`, `beautifulsoup4`
-   **Formato Datos:**
    -   Raw: JSON, CSV
    -   Bronze: Parquet (Snappy compressed)

## Ejecuci√≥n del Pipeline (Docker Flow)

Este flujo garantiza la integridad de los datos y evita problemas de permisos al separar el procesamiento de la carga.

> **Nota para usuarios Windows (sin Docker):**
> Si decides ejecutar Spark nativamente en Windows, necesitar√°s configurar `HADOOP_HOME` y descargar `winutils.exe` compatible con Hadoop 3.3. Sin esto, la escritura de archivos Parquet fallar√°. **Se recomienda encarecidamente usar Docker.**

### Paso 1: Transformaci√≥n (Generaci√≥n Local)
Ejecuta los scripts Spark para procesar los datos crudos y generar la capa Bronze localmente.

```bash
# A. Transformar NYC Data (Fuente: CSV)
docker-compose run --rm --entrypoint python elt-spark transformations/bronze/transform_nyc.py data-lake-raw-henry file:///app/bronze_output

# B. Transformar Listings (Fuente: DummyJSON API)
docker-compose run --rm --entrypoint python elt-spark transformations/bronze/transform_listings.py data-lake-raw-henry file:///app/bronze_output

# C. Transformar Posts (Fuente: JSONPlaceholder API)
docker-compose run --rm --entrypoint python elt-spark transformations/bronze/raw_to_bronze.py --raw-bucket data-lake-raw-henry --bronze-bucket file:///app/bronze_output --entity posts --source-type api
```

### Paso 2: Validaci√≥n Local (Opcional)
Verifica que los archivos Parquet se hayan generado correctamente en tu carpeta `M3/bronze_output`.

```bash
# En Windows PowerShell (Listar archivos)
dir bronze_output\marketplace_rentals\*\*\*
```

**Tip: Inspeccionar con Pandas (Python local)**
```python
import pandas as pd
# Aseg√∫rate de tener pyarrow o fastparquet instalado
df = pd.read_parquet("bronze_output/marketplace_rentals/nyc/20260122/")
print(df.head())
print(df.info())
```

### Paso 3: Carga al Data Lake
Sube los datos procesados y validados al bucket de producci√≥n en S3.

```bash
docker-compose run --rm --entrypoint python elt-spark scripts/upload_to_s3.py /app/bronze_output data-lake-bronze-henry
```
*(Reemplaza `data-lake-bronze-henry` por tu bucket definido en `.env` si es diferente)*

### Paso 4: Verificaci√≥n Final
Ejecuta el script de validaci√≥n para confirmar que los datos en S3 son legibles y consistentes.

```bash
docker-compose run --rm --entrypoint python elt-spark transformations/validation/validate_bronze.py --raw-bucket data-lake-raw-henry --bronze-bucket data-lake-bronze-henry --entity nyc
```

---

**Autor:** Ingeniero Jose David Frias
**Licencia:** MIT
